{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER = \"diffusers/examples/dreambooth/train_dreambooth.py\"\n",
    "CONVERTER = \"diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py\"\n",
    "BACK_CONVERTER = \"diffusers/scripts/convert_diffusers_to_original_stable_diffusion.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "-pGMYCm_C0_E"
   },
   "outputs": [],
   "source": [
    "#@title Login to wandb to watch training process (Optional, keep key empty if you want to skip)\n",
    "WANDB_KEY = \"\"\n",
    "if WANDB_KEY != \"\":\n",
    "  !wandb login $WANDB_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "VYQ9Mt1pizKN"
   },
   "outputs": [],
   "source": [
    "originalModels = \"/models\"\n",
    "convertModels = \"/convertModels\"\n",
    "\n",
    "# your model folder name\n",
    "targetModelName = \"animefull-final-pruned\"\n",
    "\n",
    "# if not need vae, comment it\n",
    "vae_arg = f\"--vae_path {originalModels}/animevae.pt\"\n",
    "\n",
    "#--------default variable\n",
    "SRC_PATH = originalModels + \"/\" + targetModelName\n",
    "MODEL_NAME = convertModels + \"/\" + targetModelName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDQlFaiN34Ih"
   },
   "source": [
    "## Instance Prompt and Class Prompt\n",
    "\n",
    "What your training set is about|Instance prompt must contain|Class prompt should describe\n",
    "-|-|-\n",
    "A object/person|`[V]`|The object's type and/or characteristics\n",
    "A artist's style|`by [V]`|The common characteristics of the training set\n",
    "\n",
    "Where:\n",
    "* `[V]` is a *token* in CLIP's [vocabulary](https://huggingface.co/openai/clip-vit-large-patch14/raw/main/vocab.json) which is not meaningful to the model. `sks` is a great example.\n",
    "\n",
    "A common pitfall: like if you are training about a specific person with name `[N]`, you should NOT use `[N]` as `[V]`. Names have high chance of being separated (tokenized) to multiple tokens, which is possibly hazardous.\n",
    "\n",
    "Finally `[V]` will carry the new information learned by the model.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Training about a female character:\n",
    "* Instance prompt: `sks 1girl`\n",
    "* Class prompt: `1girl`\n",
    "\n",
    "Training about hatsune miku (don't do this btw, model already knows):\n",
    "* Instance prompt: `masterpiece, best quality, sks 1girl, aqua eyes, aqua hair`\n",
    "* Class prompt: `masterpiece, best quality, 1girl, aqua eyes, aqua hair`\n",
    "\n",
    "Training about an artist's style on drawing female characters:\n",
    "* Instance prompt: `1girl, by sks`\n",
    "* Class prompt: `1girl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Weights will be saved at /train/trcoot/output\n"
     ]
    }
   ],
   "source": [
    "# declare input output folder\n",
    "\n",
    "trainingFolder = \"/train\"\n",
    "trainFolderName = \"trcoot\"\n",
    "\n",
    "#--------default variable\n",
    "INSTANCE_DIR = trainingFolder + \"/\" + trainFolderName +\"/input\"\n",
    "CLASS_DIR = trainingFolder + \"/\" + trainFolderName +\"/class\"\n",
    "OUTPUT_DIR =  trainingFolder + \"/\" + trainFolderName +\"/output\"\n",
    "\n",
    "!mkdir -p $INSTANCE_DIR\n",
    "!mkdir -p $CLASS_DIR\n",
    "!mkdir -p $OUTPUT_DIR\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Rxg0y5MBudmd"
   },
   "outputs": [],
   "source": [
    "INSTANCE_PROMPT = \"masterpiece, best quality, sks 1girl\"\n",
    "\n",
    "CLASS_PROMPT = \"masterpiece, best quality, 1girl\"\n",
    "CLASS_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
    "NUM_CLASS_IMAGES = 1\n",
    "\n",
    "SAVE_SAMPLE_PROMPT = \"masterpiece, best quality, sks 1girl, looking at viewer\"\n",
    "SAVE_SAMPLE_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn5ILIyDJIcX"
   },
   "source": [
    "## Advanced\n",
    "\n",
    "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
    "\n",
    "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
    "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
    "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
    "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
    "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
    "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
    "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
    "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
    "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n",
    "\n",
    "If you are using a GPU better than Tesla T4, remove `--gradient_checkpointing` from the arguments to improve training speed, also consider increasing `train_batch_size` for less overhead and better regularization.\n",
    "\n",
    "Remove `--use_8bit_adam` flag for full precision optimizer. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB. Somewhat unreasonable to do, but if you are encountering issues with the reduced precision, oh well.\n",
    "\n",
    "### Multiple Concepts\n",
    "\n",
    "You can set up a `concepts_list.json` like:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instance_prompt\":      \"photo of a woman wearing sks dress\",\n",
    "        \"class_prompt\":         \"photo of a woman wearing dress\",\n",
    "        \"instance_data_dir\":    \"data/wrap_dress\",\n",
    "        \"class_data_dir\":       \"data/dress\"\n",
    "    },\n",
    "    {\n",
    "        \"instance_prompt\":      \"photo of sks woman\",\n",
    "        \"class_prompt\":         \"photo of a woman\",\n",
    "        \"instance_data_dir\":    \"data/woman1\",\n",
    "        \"class_data_dir\":       \"data/woman_class\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "And use it with `--concepts_list concepts_list.json`. Can let model learn multiple concepts at the same time. Currently not compatible with Variable Prompts.\n",
    "\n",
    "### Variable Prompts\n",
    "\n",
    "For each image (`[X].png` / `[X].jpg`) in data set, put an additional `[X].txt` containing corresponding prompt with it. Then set `READ_PROMPT_FROM_TXT`. Both train set and class set supports this.\n",
    "\n",
    "Prompt read from txt `[PX]` will be inserted to the prompt you set `[P]` in train args. By default, it is inserted like `[PX] [P]`.\n",
    "\n",
    "With Variable Prompts enabled and prior preservation loss disabled (`PRIOR_PRESERVATION`), the training process is effectively an equivalent to standard finetuning.\n",
    "\n",
    "### Aspect Ratio Bucketing\n",
    "\n",
    "Used by NovelAI when they train their model. In a nutshell, it sets variable training resolution, eliminating the need of cropping dataset manually to 1:1 while still preserving information well. Brings better result especially when generating images with aspect ratio â‰  1.\n",
    "\n",
    "Cost is it slows down training and requires slightly more VRAM. Tested using it with batch size = 2 on Tesla T4 is fine.\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The default is int8 AdamW. Works well. If you want to use SGDM, prepare to get into troubles like model does not give much different results after many steps.\n",
    "\n",
    "### Storage Issue\n",
    "\n",
    "To save Colab from out of storage, by default we save unet weights using FP16 (`--save_unet_half`).\n",
    "\n",
    "If you enabled wandb, you can add `--wandb_artifact` to upload weights to wandb. Optionally, `--rm_after_wandb_saved` can let weights be removed after uploading. (Because Colab somewhat actively mess with connections, this is disabled by default.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3EAAQDUC_3lN"
   },
   "outputs": [],
   "source": [
    "#@title Advanced Parameters\n",
    "MAX_TRAIN_STEPS = 200 #@param {type:\"number\"}\n",
    "SAVE_INTERVAL = 100 #@param {type:\"number\"}\n",
    "SEED = 114514 #@param {type:\"number\"}\n",
    "#@markdown ## Data Processing\n",
    "RESOLUTION = 512 #@param {type:\"slider\", min:64, max:2048, step:28}\n",
    "ASPECT_RATIO_BUCKETING = False #@param {type:\"boolean\"}\n",
    "READ_PROMPT_FROM_TXT = \"instance\" #@param [\"no\", \"instance\", \"class\", \"both\"] {allow-input: false}\n",
    "#@markdown ## Forward Pass\n",
    "TRAIN_BATCH_SIZE = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "CLIP_SKIP = 2 #@param {type:\"slider\", min:1, max:6, step:1}\n",
    "MIXED_PRECISION = \"fp16\" #@param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
    "#@markdown ## Optimizer / Backward Pass\n",
    "OPTIMIZER = \"adamw_8bit\" #@param [\"adamw\", \"adamw_8bit\", \"adamw_ds\", \"sgdm\", \"sgdm_8bit\"] {allow-input: false}\n",
    "LEARNING_RATE = 5e-6 #@param {type:\"number\"}\n",
    "LR_SCHEDULER = \"cosine_with_restarts\"  #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
    "LR_WARMUP_STEPS = 100 #@param {type:\"number\"}\n",
    "LR_CYCLES = 1 #@param {type:\"number\"}\n",
    "LAST_EPOCH = -1 #@param {type:\"number\"}\n",
    "SCALE_LR = True #@param {type:\"boolean\"}\n",
    "PRIOR_PRESERVATION = True #@param {type:\"boolean\"}\n",
    "PRIOR_LOSS_WEIGHT = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "#@markdown ## Inference (Class Set Generation / Sample Images Generation)\n",
    "INFER_STEPS = 28 #@param {type:\"integer\"}\n",
    "GUIDANCE_SCALE = 11 #@param {type:\"integer\"}\n",
    "SAMPLE_N = 4  #@param {type:\"integer\"}\n",
    "INFER_BATCH_SIZE = 2 #@param {type:\"slider\", min:1, max:10, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "G26jPrUSqWUr"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ~/.cache/huggingface/accelerate\n",
    "\n",
    "cat > ~/.cache/huggingface/accelerate/default_config.yaml <<- EOM\n",
    "compute_environment: LOCAL_MACHINE\n",
    "deepspeed_config: {}\n",
    "distributed_type: 'NO'\n",
    "downcast_bf16: 'no'\n",
    "fsdp_config: {}\n",
    "machine_rank: 0\n",
    "main_process_ip: null\n",
    "main_process_port: null\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "use_cpu: false\n",
    "EOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcSXTp-u-Eg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_cpu_threads_per_process` was set to `8` to improve out-of-box performance\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "Caching latents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.04s/it]\n",
      "Steps:  33%|â–Ž| 66/200 [00:57<01:41,  1.32step/s, epoch=66, loss=0.0694, lr=3.3e-"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!mkdir -p $OUTPUT_DIR\n",
    "\n",
    "wandb_arg = \"--wandb\" if WANDB_KEY != \"\" else \"\"\n",
    "scale_lr_arg = \"--scale_lr\" if SCALE_LR else \"\"\n",
    "ppl_arg = f\"--with_prior_preservation --prior_loss_weight={PRIOR_LOSS_WEIGHT}\" if PRIOR_PRESERVATION else \"\"\n",
    "read_prompt_arg = f\"--read_prompt_from_txt {READ_PROMPT_FROM_TXT}\" if READ_PROMPT_FROM_TXT != \"no\" else \"\"\n",
    "arb_arg = \"--use_aspect_ratio_bucket --debug_arb\" if ASPECT_RATIO_BUCKETING else \"\"\n",
    "\n",
    "\n",
    "!accelerate launch $TRAINER \\\n",
    "  --instance_data_dir \"{INSTANCE_DIR}\" \\\n",
    "  --instance_prompt \"{INSTANCE_PROMPT}\" \\\n",
    "  --pretrained_model_name_or_path \"{MODEL_NAME}\" \\\n",
    "  --pretrained_vae_name_or_path \"{MODEL_NAME}/vae\" \\\n",
    "  --output_dir \"{OUTPUT_DIR}\" \\\n",
    "  --seed=$SEED \\\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --optimizer \"{OPTIMIZER}\" \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --lr_scheduler=$LR_SCHEDULER \\\n",
    "  --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
    "  --lr_cycles=$LR_CYCLES \\\n",
    "  --last_epoch=$LAST_EPOCH \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "  --save_interval=$SAVE_INTERVAL \\\n",
    "  --class_data_dir \"{CLASS_DIR}\" \\\n",
    "  --class_prompt \"{CLASS_PROMPT}\" --class_negative_prompt \"{CLASS_NEGATIVE_PROMPT}\" \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --save_sample_prompt \"{SAVE_SAMPLE_PROMPT}\" --save_sample_negative_prompt \"{SAVE_SAMPLE_NEGATIVE_PROMPT}\" \\\n",
    "  --n_save_sample=$SAMPLE_N \\\n",
    "  --infer_batch_size=$INFER_BATCH_SIZE \\\n",
    "  --infer_steps=$INFER_STEPS \\\n",
    "  --guidance_scale=$GUIDANCE_SCALE \\\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --gradient_checkpointing \\\n",
    "  --save_unet_half \\\n",
    "  --mixed_precision \"{MIXED_PRECISION}\" \\\n",
    "  --clip_skip=$CLIP_SKIP \\\n",
    "  $wandb_arg $scale_lr_arg $ppl_arg $read_prompt_arg $arb_arg\n",
    "\n",
    "# disabled: --not_cache_latents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8wgU0HN-Kq"
   },
   "source": [
    "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "#@markdown Which step number to use.\n",
    "use_checkpoint = '2000' #@param {type:\"string\"}\n",
    "#@markdown Id of which run to use (empty = latest run).\n",
    "run_id = '' #@param {type:\"string\"}\n",
    "\n",
    "if not run_id:\n",
    "  runs = [d for d in Path(OUTPUT_DIR).iterdir() if d.is_dir()]\n",
    "  runs.sort(lambda d: d.stat().st_ctime, reverse=True)\n",
    "  run_id = runs[0].name\n",
    "\n",
    "ckpt_path = f'{OUTPUT_DIR}/{run_id}/{use_checkpoint}/model.ckpt'\n",
    "\n",
    "# You can add --vae and --text_encoder if you want.\n",
    "!python \"{BACK_CONVERTER}\" --model_path \"{OUTPUT_DIR}/{run_id}/{use_checkpoint}\" --checkpoint_path $ckpt_path \\\n",
    "  --unet_dtype fp16\n",
    "\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XU7NuMAA2drw"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLWXPZqjsZVV"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CCRcmcpe/diffusers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model...\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert succeed\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!echo \"Converting model...\"\n",
    "!python $CONVERTER --checkpoint_path $SRC_PATH/model.ckpt --original_config_file $SRC_PATH/config.yaml $vae_arg --dump_path $MODEL_NAME --scheduler_type ddim\n",
    "!echo \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
