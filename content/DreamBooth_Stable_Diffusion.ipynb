{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER = \"diffusers/examples/dreambooth/train_dreambooth.py\"\n",
    "BACK_CONVERTER = \"diffusers/scripts/convert_diffusers_to_original_stable_diffusion.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-pGMYCm_C0_E"
   },
   "outputs": [],
   "source": [
    "#@title Login to wandb to watch training process (Optional, keep key empty if you want to skip)\n",
    "WANDB_KEY = \"\"\n",
    "if WANDB_KEY != \"\":\n",
    "  !wandb login $WANDB_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VYQ9Mt1pizKN"
   },
   "outputs": [],
   "source": [
    "#declare your model name\n",
    "modelName = \"animefull-final-pruned\"\n",
    "\n",
    "#default models location\n",
    "originalModels = \"/models\"\n",
    "convertModels = \"/convertModels\"\n",
    "\n",
    "# if not need vae, comment it\n",
    "vae_arg = f\"--vae_path {originalModels}/animevae.pt\"\n",
    "\n",
    "#--------default variable\n",
    "SRC_PATH = originalModels + \"/\" + modelName\n",
    "MODEL_NAME = convertModels + \"/\" + modelName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDQlFaiN34Ih"
   },
   "source": [
    "## Instance Prompt and Class Prompt\n",
    "\n",
    "What your training set is about|Instance prompt must contain|Class prompt should describe\n",
    "-|-|-\n",
    "A object/person|`[V]`|The object's type and/or characteristics\n",
    "A artist's style|`by [V]`|The common characteristics of the training set\n",
    "\n",
    "Where:\n",
    "* `[V]` is a *token* in CLIP's [vocabulary](https://huggingface.co/openai/clip-vit-large-patch14/raw/main/vocab.json) which is not meaningful to the model. `sks` is a great example.\n",
    "\n",
    "A common pitfall: like if you are training about a specific person with name `[N]`, you should NOT use `[N]` as `[V]`. Names have high chance of being separated (tokenized) to multiple tokens, which is possibly hazardous.\n",
    "\n",
    "Finally `[V]` will carry the new information learned by the model.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Training about a female character:\n",
    "* Instance prompt: `sks 1girl`\n",
    "* Class prompt: `1girl`\n",
    "\n",
    "Training about hatsune miku (don't do this btw, model already knows):\n",
    "* Instance prompt: `masterpiece, best quality, sks 1girl, aqua eyes, aqua hair`\n",
    "* Class prompt: `masterpiece, best quality, 1girl, aqua eyes, aqua hair`\n",
    "\n",
    "Training about an artist's style on drawing female characters:\n",
    "* Instance prompt: `1girl, by sks`\n",
    "* Class prompt: `1girl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare input output folder\n",
    "\n",
    "trainingFolder = \"/train\"\n",
    "trainFolderName = \"trcoot\"\n",
    "\n",
    "#--------default variable\n",
    "INSTANCE_DIR = trainingFolder + \"/\" + trainFolderName +\"/input\"\n",
    "CLASS_DIR = trainingFolder + \"/\" + trainFolderName +\"/class\"\n",
    "OUTPUT_DIR =  trainingFolder + \"/\" + trainFolderName +\"/output\"\n",
    "\n",
    "!mkdir -p $INSTANCE_DIR\n",
    "!mkdir -p $CLASS_DIR\n",
    "!mkdir -p $OUTPUT_DIR\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxg0y5MBudmd"
   },
   "outputs": [],
   "source": [
    "INSTANCE_PROMPT = \"1girl, by bala, \"\n",
    "\n",
    "CLASS_PROMPT = \"1girl\"\n",
    "CLASS_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
    "NUM_CLASS_IMAGES = 25\n",
    "\n",
    "SAVE_SAMPLE_PROMPT = \"masterpiece, best quality, 1girl, by bala, looking at viewer\"\n",
    "SAVE_SAMPLE_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EAAQDUC_3lN"
   },
   "outputs": [],
   "source": [
    "#@title Advanced Parameters\n",
    "MAX_TRAIN_STEPS = 2000 #@param {type:\"number\"}\n",
    "SAVE_INTERVAL = 500 #@param {type:\"number\"}\n",
    "SEED = 114514 #@param {type:\"number\"}\n",
    "#@markdown ## Data Processing\n",
    "RESOLUTION = 512 #@param {type:\"slider\", min:64, max:2048, step:28}\n",
    "ASPECT_RATIO_BUCKETING = False #@param {type:\"boolean\"}\n",
    "READ_PROMPT_FROM_TXT = \"instance\" #@param [\"no\", \"instance\", \"class\", \"both\"] {allow-input: false}\n",
    "#@markdown ## Forward Pass\n",
    "TRAIN_BATCH_SIZE = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "CLIP_SKIP = 2 #@param {type:\"slider\", min:1, max:6, step:1}\n",
    "MIXED_PRECISION = \"fp16\" #@param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
    "#@markdown ## Optimizer / Backward Pass\n",
    "OPTIMIZER = \"adamw_8bit\" #@param [\"adamw\", \"adamw_8bit\", \"adamw_ds\", \"sgdm\", \"sgdm_8bit\"] {allow-input: false}\n",
    "LEARNING_RATE = 5e-6 #@param {type:\"number\"}\n",
    "LR_SCHEDULER = \"cosine_with_restarts\"  #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
    "LR_WARMUP_STEPS = 100 #@param {type:\"number\"}\n",
    "LR_CYCLES = 1 #@param {type:\"number\"}\n",
    "LAST_EPOCH = -1 #@param {type:\"number\"}\n",
    "SCALE_LR = True #@param {type:\"boolean\"}\n",
    "PRIOR_PRESERVATION = False #@param {type:\"boolean\"}\n",
    "PRIOR_LOSS_WEIGHT = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "#@markdown ## Inference (Class Set Generation / Sample Images Generation)\n",
    "INFER_STEPS = 28 #@param {type:\"integer\"}\n",
    "GUIDANCE_SCALE = 11 #@param {type:\"integer\"}\n",
    "SAMPLE_N = 4  #@param {type:\"integer\"}\n",
    "INFER_BATCH_SIZE = 1 #@param {type:\"slider\", min:1, max:10, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G26jPrUSqWUr"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ~/.cache/huggingface/accelerate\n",
    "\n",
    "cat > ~/.cache/huggingface/accelerate/default_config.yaml <<- EOM\n",
    "compute_environment: LOCAL_MACHINE\n",
    "deepspeed_config: {}\n",
    "distributed_type: 'NO'\n",
    "downcast_bf16: 'no'\n",
    "fsdp_config: {}\n",
    "machine_rank: 0\n",
    "main_process_ip: null\n",
    "main_process_port: null\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "use_cpu: false\n",
    "EOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcSXTp-u-Eg"
   },
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!mkdir -p $OUTPUT_DIR\n",
    "\n",
    "wandb_arg = \"--wandb\" if WANDB_KEY != \"\" else \"\"\n",
    "scale_lr_arg = \"--scale_lr\" if SCALE_LR else \"\"\n",
    "ppl_arg = f\"--with_prior_preservation --prior_loss_weight={PRIOR_LOSS_WEIGHT}\" if PRIOR_PRESERVATION else \"\"\n",
    "read_prompt_arg = f\"--read_prompt_from_txt {READ_PROMPT_FROM_TXT}\" if READ_PROMPT_FROM_TXT != \"no\" else \"\"\n",
    "arb_arg = \"--use_aspect_ratio_bucket --debug_arb\" if ASPECT_RATIO_BUCKETING else \"\"\n",
    "\n",
    "\n",
    "!accelerate launch $TRAINER \\\n",
    "  --instance_data_dir \"{INSTANCE_DIR}\" \\\n",
    "  --instance_prompt \"{INSTANCE_PROMPT}\" \\\n",
    "  --pretrained_model_name_or_path \"{MODEL_NAME}\" \\\n",
    "  --pretrained_vae_name_or_path \"{MODEL_NAME}/vae\" \\\n",
    "  --output_dir \"{OUTPUT_DIR}\" \\\n",
    "  --seed=$SEED \\\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --optimizer \"{OPTIMIZER}\" \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --lr_scheduler=$LR_SCHEDULER \\\n",
    "  --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
    "  --lr_cycles=$LR_CYCLES \\\n",
    "  --last_epoch=$LAST_EPOCH \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "  --save_interval=$SAVE_INTERVAL \\\n",
    "  --class_data_dir \"{CLASS_DIR}\" \\\n",
    "  --class_prompt \"{CLASS_PROMPT}\" --class_negative_prompt \"{CLASS_NEGATIVE_PROMPT}\" \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --save_sample_prompt \"{SAVE_SAMPLE_PROMPT}\" --save_sample_negative_prompt \"{SAVE_SAMPLE_NEGATIVE_PROMPT}\" \\\n",
    "  --n_save_sample=$SAMPLE_N \\\n",
    "  --infer_batch_size=$INFER_BATCH_SIZE \\\n",
    "  --infer_steps=$INFER_STEPS \\\n",
    "  --guidance_scale=$GUIDANCE_SCALE \\\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --gradient_checkpointing \\\n",
    "  --save_unet_half \\\n",
    "  --mixed_precision \"{MIXED_PRECISION}\" \\\n",
    "  --clip_skip=$CLIP_SKIP \\\n",
    "  --sample_interval=$SAVE_INTERVAL \\\n",
    "  $wandb_arg $scale_lr_arg $ppl_arg $read_prompt_arg $arb_arg\n",
    "\n",
    "# disabled: --not_cache_latents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8wgU0HN-Kq"
   },
   "source": [
    "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "#@markdown Which step number to use.\n",
    "use_checkpoint = '2000' #@param {type:\"string\"}\n",
    "#@markdown Id of which run to use (empty = latest run).\n",
    "run_id = '' #@param {type:\"string\"}\n",
    "\n",
    "if not run_id:\n",
    "  runs = [d for d in Path(OUTPUT_DIR).iterdir() if d.is_dir()]\n",
    "  runs.sort(lambda d: d.stat().st_ctime, reverse=True)\n",
    "  run_id = runs[0].name\n",
    "\n",
    "ckpt_path = f'{OUTPUT_DIR}/{run_id}/{use_checkpoint}/model.ckpt'\n",
    "\n",
    "# You can add --vae and --text_encoder if you want.\n",
    "!python \"{BACK_CONVERTER}\" --model_path \"{OUTPUT_DIR}/{run_id}/{use_checkpoint}\" --checkpoint_path $ckpt_path \\\n",
    "  --unet_dtype fp16\n",
    "\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLWXPZqjsZVV"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CCRcmcpe/diffusers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
